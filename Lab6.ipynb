{"cells":[{"cell_type":"markdown","metadata":{"id":"d3cd831b-b67e-4d49-a8d1-446eef297ad9"},"source":["## TI3160TU: Natural Language Processing - Transformers and BERT models Lab"]},{"cell_type":"markdown","metadata":{"id":"c4908bae-2761-4304-8b78-34d0969c9f4d"},"source":["In this hands-on lab, we will explore BERT models. As we have seen in the lecture, BERT models are based on Transformers and allow us to extract contextual embedding as well as perform various NLP tasks by either acting as a feature extractor or by fine-tuning the model and using it for a specific task. In this lab, we will explore three main aspects of BERT models:\n","\n","1. **Basic Functionalities of BERT models**\n","2. **Performing NLP Classification tasks using BERT models as a feature extractor**\n","3. **Performing NLP Classification tasks by fine-tuning BERT models and training a classification head**\n","\n","For the purposes of this lab, we are going to use DistilBERT instead of the original version of BERT released by Google. The main reason for using DistilBERT is that it is a ligher and faster variant of BERT, designed for cases where computational resources or speed is constrained. Nevertheless, DistilBERT retains 97% of BERT performance, while using 40% fewer parameters. All the aspects that we will see in this lab apply to the original BERT model as well."]},{"cell_type":"markdown","metadata":{"id":"561020e3-aedd-4f03-81d5-f56362528327"},"source":["### 1. Basic Functionalities of BERT models"]},{"cell_type":"markdown","metadata":{"id":"bfe927c3-20a8-4a52-b3ff-3fb282fb8b77"},"source":["We will start this lab by demonstrating some basic functionalities of BERT models. We will focus on:\n","    \n","1. Extracting Contextual Word Embeddings\n","2. Extracting Contextual Sentence/Document Embeddings"]},{"cell_type":"markdown","metadata":{"id":"21f1c2eb-d53a-4364-bdd2-8bcd48db19f3"},"source":["### 1.1. Extracting Contextual Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"bc35d385-f9ea-4b77-a202-ee5674ac3f54"},"source":["We begin by demonstrating how we can leverage BERT-based models to extract contextual embeddings for words/documents in our dataset. We will do the following steps:\n","1. Load a BERT-based model using the Transformers library. As mentioned above, we will use DistilBERT.\n","2. Extract contextual word embeddings\n","4. Demonstrate how the same tokens have different embeddings based on the provided context (i.e., surrounding words).\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T15:45:06.037986Z","iopub.status.busy":"2023-12-13T15:45:06.037697Z","iopub.status.idle":"2023-12-13T15:45:30.577860Z","shell.execute_reply":"2023-12-13T15:45:30.576741Z","shell.execute_reply.started":"2023-12-13T15:45:06.037961Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n"]}],"source":["!pip install transformers\n","!pip install tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:30.583721Z","iopub.status.busy":"2023-12-13T15:45:30.583368Z","iopub.status.idle":"2023-12-13T15:45:34.172176Z","shell.execute_reply":"2023-12-13T15:45:34.170961Z","shell.execute_reply.started":"2023-12-13T15:45:30.583681Z"},"id":"d7de2fcd-3e04-463e-88f7-065f1881a1bf","outputId":"14ecc77c-cb2a-475e-d954-0114b1c3715f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example 1: I hit the ball with a baseball bat. Shape of Embeddings for Example 1: torch.Size([11, 768])\n","Example 2: I was scared because I saw a bat. Shape of Embeddings for Example 2: torch.Size([11, 768])\n"]}],"source":["# demonstrate basic stuff\n","# 1. Extract Contextual Word Embeddings\n","# 2. Extract classification embeddings\n","from transformers import DistilBertTokenizer, DistilBertModel\n","import torch\n","\n","# Load DistilBERT\n","distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","# Define two example documents. These are the examples included in the Lecture\n","example1 = \"I hit the ball with a baseball bat.\"\n","example2 = \"I was scared because I saw a bat.\"\n","\n","\n","# function to extract contextual embeddings from BERT based models\n","# INPUT: The trained BERT-model, its tokenizer function, and the document that we want to get embeddings for\n","# OUTPUT: A tensor with all the embeddings extracted from the BERT model\n","def extract_embeddings(model, tokenizer, text):\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    embeddings = outputs.last_hidden_state.squeeze(0)\n","    return embeddings\n","\n","\n","# extract the embeddings for our examples\n","example1_embeddings = extract_embeddings(distilbert_model, distilbert_tokenizer, example1)\n","example2_embeddings = extract_embeddings(distilbert_model, distilbert_tokenizer, example2)\n","\n","# lets inspect the shapes of our embeddings\n","print(f\"Example 1: {example1} Shape of Embeddings for Example 1: {example1_embeddings.shape}\")\n","print(f\"Example 2: {example2} Shape of Embeddings for Example 2: {example2_embeddings.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"20be5144-6bd9-4f1a-89b7-c6c35312d036"},"source":["We observe that for each of our examples, we obtained a Pytorch tensor that includes 11 embeddings (X-dimension), with each embedding having a dimension of 768 (DistilBERT generates embeddings with size of 768 dimensions).\n","\n","The question here is what each of these 11 embeddings correspond to. By looking at our examples, we observe that both documents consists of 8 words and 1 punctuation character. So you might be wondering why do we get 11 embeddings. To demistify this, lets look how our BERT-based model sees these examples after tokenization."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:34.173788Z","iopub.status.busy":"2023-12-13T15:45:34.173331Z","iopub.status.idle":"2023-12-13T15:45:34.182941Z","shell.execute_reply":"2023-12-13T15:45:34.181765Z","shell.execute_reply.started":"2023-12-13T15:45:34.173759Z"},"id":"e5e73850-e10b-4902-9f10-bbf964f91799","outputId":"00debf4f-1e54-4528-8fcf-e16ae09d1ed1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example 1: I hit the ball with a baseball bat. Tokenized Example 1 using DistilBERT tokenizer: ['[CLS]', 'i', 'hit', 'the', 'ball', 'with', 'a', 'baseball', 'bat', '.', '[SEP]']\n","Example 2: I was scared because I saw a bat. Tokenized Example 2 using DistilBERT tokenizer: ['[CLS]', 'i', 'was', 'scared', 'because', 'i', 'saw', 'a', 'bat', '.', '[SEP]']\n"]}],"source":["#helper function that tokenizes a document based on the tokenizer function of our BERT model\n","#INPUT: the tokenizer function from the BERT model and the document that we want to tokenize\n","#OUTPUT: a list of tokens\n","def extract_tokens(tokenizer_function, text):\n","    inputs = tokenizer_function(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, padding=True)\n","    tokens = tokenizer_function.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","    return tokens\n","\n","# print the tokens of each example based on DistilBERT tokenizer\n","print(f\"Example 1: {example1} Tokenized Example 1 using DistilBERT tokenizer: {extract_tokens(distilbert_tokenizer, example1)}\")\n","print(f\"Example 2: {example2} Tokenized Example 2 using DistilBERT tokenizer: {extract_tokens(distilbert_tokenizer, example2)}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5b74ab9f-f21f-4762-8c9b-733c11aa527f"},"source":["Remember that BERT-based models use a sub-word tokenizer and also include some special tokens, such as [CLS] and [SEP]. BY tokenizing our examples using our model's tokenizer, we can observe that indeed each example consists of 11 tokens (each token has one embedding). For instance, for example 1:\n","1. In position 1: Embedding of token **[CLS]**\n","2. In position 2: Embedding of token **i**\n","3. In position 3: Embedding of token **hit**\n","4. In position 4: Embedding of token **the**\n","5. In position 5: Embedding of token **ball**\n","6. In position 6: Embedding of token **with**\n","7. In position 7: Embedding of token **a**\n","8. In position 8: Embedding of token **baseball**\n","9. In position 9: Embedding of token **bat**\n","10. In position 10: Embedding of token **.**\n","11. In position 11: Embedding of token **[SEP]**\n","\n","\n","As a reminder:\n","\n","1. [CLS]: Stands for \"classification\". In BERT, every input sequence starts with this token. After processing the sequence, the embedding of this token is often used as a representation for the entire sequence. Especially after fine-tuning on a classification task, the [CLS] embedding captures aggregate information of the sequence, making it suitable for sequence-level predictions. Without fine-tuning we usually do not use this token, as it doesn't have much meaning.\n","\n","2. [SEP]: Stands for \"separator\". This token is used in BERT to separate two sequences when the model takes in a pair of sequences as input. For instance, in tasks like question answering or sentence-pair classification, the two sequences (e.g., question and context or sentence1 and sentence2) are separated by a [SEP] token to indicate the end of one sequence and the beginning of another."]},{"cell_type":"markdown","metadata":{"id":"1ecc3316-c161-4a32-8ac9-abdaca91a595"},"source":["Now, lets inspect our embeddings and demonstrate that they are contextual (Note that these embeddings are substantially better and different that the embeddings that we saw in Word2vec model that are generated in a context-free manner). Lets start first with the token \"bat\". Both examples include the same word, however, the word has different meanings in the two provided example. We expect that the generated embeddings for \"bat\" in the two examples will be substantially different to capture that in one of the example we mean a baseball bat, while on the other we mean bat the mammal.\n","\n","The token \"bat\" appears in position 8 for both examples. So lets compare the two embeddings that are provided in position 8...\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:34.185879Z","iopub.status.busy":"2023-12-13T15:45:34.185588Z","iopub.status.idle":"2023-12-13T15:45:34.199206Z","shell.execute_reply":"2023-12-13T15:45:34.198235Z","shell.execute_reply.started":"2023-12-13T15:45:34.185844Z"},"id":"5891a87d-3742-4243-acbe-c5bfe24c0635","outputId":"3da1d58e-5de9-4506-f90a-2751632aa464","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity betweeen the two embeddings for the token bat 0.8347097635269165\n"]}],"source":["# necessary import to calculate metrics such as Cosine similarity between tensors\n","import torch.nn.functional as F\n","\n","# extract the embeddings corresponding to the token \"bat\" in these two examples\n","bat_embedding_example1 = example1_embeddings[8]\n","bat_embedding_example2 = example2_embeddings[8]\n","\n","# Calculate cosine similarity using Pytorch's function\n","similarity = F.cosine_similarity(bat_embedding_example1.unsqueeze(0), bat_embedding_example2.unsqueeze(0)).item()\n","\n","# print the similarity\n","print(f\"Similarity betweeen the two embeddings for the token bat {similarity}\")"]},{"cell_type":"markdown","metadata":{"id":"e2d106a6-b88e-41cd-90ac-9cf9904aaf6a"},"source":["We observe that these embeddings have a cosine similarity of 0.83. This indicates that the BERT model, based on these document, the token is quite similar across the two documents, however, the embeddings are different based on the context (i.e., the surrounding words). Remember that in Word2vec the word \"bat\" will have the same embedding in both example, hence having a cosine similarity of 1.0."]},{"cell_type":"markdown","metadata":{"id":"f61313e0-3d65-4896-b60a-07e94504a39b"},"source":["Thus far, we have demonstrated that the same token can have different embeddings in different documents. But can a token have different embeddings within the same document? For instance, in example 2, the token \"i\" appears twice. We expect that this token will have different embeddings because of the Transformer's attention mechanism. Lets demonstrate this with the token \"i\" that appears in positions 1 and 5 in example 2."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:34.201007Z","iopub.status.busy":"2023-12-13T15:45:34.200625Z","iopub.status.idle":"2023-12-13T15:45:34.211387Z","shell.execute_reply":"2023-12-13T15:45:34.210534Z","shell.execute_reply.started":"2023-12-13T15:45:34.200982Z"},"id":"af6dacbd-47e2-4539-a832-75f996d6c59b","outputId":"b3b7d385-afc7-4b20-f035-59ea8474f2bf","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity betweeen the two embeddings for the token i in example 2 0.7227323651313782\n"]}],"source":["# extract the embeddings corresponding to the token \"i\" in example 2\n","i1_embedding_example2 = example2_embeddings[1]\n","i2_embedding_example2 = example2_embeddings[5]\n","\n","# Calculate cosine similarity using Pytorch's function\n","similarity = F.cosine_similarity(i1_embedding_example2.unsqueeze(0), i2_embedding_example2.unsqueeze(0)).item()\n","\n","# print the similarity\n","print(f\"Similarity betweeen the two embeddings for the token i in example 2 {similarity}\")"]},{"cell_type":"markdown","metadata":{"id":"b92f085f-bb2a-46b0-bb3a-953211e73db2"},"source":["We observe that these embeddings have a cosine similarity of 0.72. Despite being part of the same document and being the same exact token, the generated embeddings are different based on how the tokens are used in the document!"]},{"cell_type":"markdown","metadata":{"id":"b9301a17-a05a-4aac-8879-dc2dc9a929db"},"source":["### 1.2. Extracting Contextual Sentence Embeddings"]},{"cell_type":"markdown","metadata":{"id":"f2a3bc3d-4183-4292-b405-61e232d0fe6b"},"source":["BERT and its variants are primarily designed to generate token-level embeddings, but there are several strategies to derive sentence-level embeddings from these token-level embeddings. The three most used are:\n","\n","- **[CLS] Token Embedding:** After feeding a sentence through BERT, the embedding corresponding to the [CLS] token can be used as the sentence representation. This method works well when BERT is fine-tuned on a downstream classification task since the [CLS] token is trained to aggregate sequence-level information for such tasks. However, for the pre-trained BERT model without any fine-tuning, the [CLS] embedding may not be the best representative for the whole sentence.\n","\n","- **Mean/Average Pooling:** Take the average of all token embeddings in the sequence. This approach gives equal importance to all tokens, which might not always be the best representation, especially if the sentence is long.\n","\n","- **Sentence-BERT (SBERT):** SBERT is a modification of the pre-trained BERT to derive fixed-size sentence embeddings. It uses a siamese or triplet network structure to derive semantically meaningful sentence embeddings that are then fine-tuned on various NLP tasks. The embeddings from SBERT can be used directly for sentence-level comparisons and are optimized to be more semantically meaningful than naive BERT embeddings. See https://www.sbert.net/ for more details.\n","\n","\n","For the purposes of this lab, at this stage, we are going to use the second and third approach to generate sentence embeddings. Later on the lab, we will fine-tune DistilBERT and use the [CLS] token to perform classification."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:34.212797Z","iopub.status.busy":"2023-12-13T15:45:34.212471Z","iopub.status.idle":"2023-12-13T15:45:34.299746Z","shell.execute_reply":"2023-12-13T15:45:34.298577Z","shell.execute_reply.started":"2023-12-13T15:45:34.212767Z"},"id":"ea7e5ce6-c5c3-4b9a-ac03-b6908daa4532","outputId":"75471426-8983-4319-927f-f12325bc64dc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity betweeen the sentence embeddings of our two examples 0.8488353490829468\n"]}],"source":["# Define a function to get embeddings\n","def get_document_embeddings(model, tokenizer, text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n","    outputs = model(**inputs)\n","    return outputs.last_hidden_state.mean(dim=1)  # Get the mean of the token embeddings as a document representation\n","\n","example1_sentence_embedding = get_document_embeddings(distilbert_model, distilbert_tokenizer, example1)\n","example2_sentence_embedding = get_document_embeddings(distilbert_model, distilbert_tokenizer, example2)\n","\n","# Calculate cosine similarity using Pytorch's function\n","similarity = F.cosine_similarity(example1_sentence_embedding, example2_sentence_embedding).item()\n","\n","# print the similarity\n","print(f\"Similarity betweeen the sentence embeddings of our two examples {similarity}\")\n"]},{"cell_type":"markdown","metadata":{"id":"b21b11d7-e1da-4068-86cb-0fefa79eaf19"},"source":["We observe that by using the mean of the word embeddings we obtain a high cosine similarity (0.84) between the two examples, while in fact they are quite semantically different. Lets try to do the same thing, but using Sentence Transformers that are more suitable for assessing the semantic similarity across sentences/documents."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:34.301535Z","iopub.status.busy":"2023-12-13T15:45:34.301250Z","iopub.status.idle":"2023-12-13T15:45:58.710694Z","shell.execute_reply":"2023-12-13T15:45:58.709482Z","shell.execute_reply.started":"2023-12-13T15:45:34.301510Z"},"id":"OUd6LMnAlpGn","outputId":"cc70a044-9625-4b61-cc46-053d4ded5d61","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.36.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.19.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install sentence_transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T15:45:58.712541Z","iopub.status.busy":"2023-12-13T15:45:58.712221Z","iopub.status.idle":"2023-12-13T15:46:02.576673Z","shell.execute_reply":"2023-12-13T15:46:02.575671Z","shell.execute_reply.started":"2023-12-13T15:45:58.712512Z"},"id":"dcf7635b-d200-457c-ae64-aba185807b8e","outputId":"b66cd02e-6383-486a-f9fb-9cd299d73d21","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dac931fc60954a48a1f2aceba541ec53","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Similarity betweeen the sentence embeddings of our two examples 0.6303306818008423\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","\n","# Load the distilbert model trained for sentence embeddings\n","model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n","\n","# Create a list of sentences\n","sentences = [example1, example2]\n","\n","# Get embeddings for the sentences\n","embeddings = model.encode(sentences)\n","\n","# Calculate cosine similarity using Pytorch's function\n","similarity = F.cosine_similarity(torch.from_numpy(embeddings[0]).unsqueeze(0), torch.from_numpy(embeddings[1]).unsqueeze(0)).item()\n","\n","# print the similarity\n","print(f\"Similarity betweeen the sentence embeddings of our two examples {similarity}\")\n"]},{"cell_type":"markdown","metadata":{"id":"2c06c776-c7fd-4bc7-a74c-785bdda3dccb"},"source":["We observe that by using the Sentence Transformers model, we obtain a lower cosine similarity (0.63) between the two examples than before. Overall, for semantic similarity tasks on the Sentence/Document model is better to use Sentence Transformers model like the one we used here."]},{"cell_type":"markdown","metadata":{"id":"5b45f10a-db5d-4be7-93b9-04d23c145c49"},"source":["### 2. Performing NLP Classification tasks using BERT models as a feature extractor"]},{"cell_type":"markdown","metadata":{"id":"81c8232b-321a-431f-ab3e-5ca4cbeb7f66"},"source":["Here, we will demonstrate how we can use BERT to solve classification tasks where BERT models will be solely used for feature extraction. Specifically, we will implement an NLP pipeline that takes as an input a movie review (in text), then we will use BERT models (specifically DistilBERT) to convert the raw text into a dense vector representation that encapsulates the semantics of the review. Then, we will use traditional ML classifiers (e.g., Logistic Regression) to perform classiciation on the movie reviews."]},{"cell_type":"markdown","metadata":{"id":"24ca6d28-0960-4923-b85c-be8bf8bb31d9","tags":[]},"source":["#### 2.1 Extracting representations from BERT models"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T15:46:02.578416Z","iopub.status.busy":"2023-12-13T15:46:02.578084Z","iopub.status.idle":"2023-12-13T16:02:28.059463Z","shell.execute_reply":"2023-12-13T16:02:28.058339Z","shell.execute_reply.started":"2023-12-13T15:46:02.578387Z"},"id":"947d1bdf-0c34-43c7-9cb9-64552045166f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from datasets import load_dataset\n","import pandas as pd\n","import numpy as np\n","\n","# lets use GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","\n","# Load Distilbert and move to GPU\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n","\n","# function to extract the sentence embeddings\n","def sentences_to_embeddings(sentences, tokenizer, model, device):\n","    # Tokenize a batch of sentences and prepare the tensors\n","    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    \n","    embeddings = outputs.last_hidden_state\n","    mean_embeddings = torch.mean(embeddings, dim=1)\n","    return mean_embeddings.cpu()  # Move embeddings back to CPU if necessary\n","\n","\n","# load the IMDB reviews dataset\n","df = pd.read_csv('movie.csv')\n","reviews = df['text'].tolist()\n","\n","\n","# we extract the embeddings in batches to avoid memory issues\n","chunk_size = 100  # Adjust based on GPU memory\n","embeddings = []\n","for i in range(0, len(df), chunk_size):\n","    batch = df['text'][i:i + chunk_size].tolist()\n","    batch_embeddings = sentences_to_embeddings(batch, tokenizer, model, device)\n","    embeddings.extend(batch_embeddings)\n","    \n","\n","# Convert embeddings to numpy for easy handling\n","embeddings = [embedding.detach().numpy() for embedding in embeddings]\n","\n","# convert all embeddings to a 2D array that is our input\n","X= np.vstack(embeddings)\n","\n","# our class labels which is the output of the classification task (ground truth)\n","Y = df['label'].tolist()"]},{"cell_type":"markdown","metadata":{"id":"56a8d74a-8831-4cac-9ff5-0a8ae9093869"},"source":["#### 2.2 Sentiment Classification using BERT representations and Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"40ec1384-22e5-4a05-bb26-fefca1888486"},"source":["Having extracted dense representations for each review in our dataset, now we are going to proceed and train a classifier using Logistic Regression. This part is identical to what we saw in the Vector Semantics Lab; the only difference is that here we are using BERT representations instead of TF-IDF representations."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-13T16:02:28.061639Z","iopub.status.busy":"2023-12-13T16:02:28.061319Z","iopub.status.idle":"2023-12-13T16:02:44.161386Z","shell.execute_reply":"2023-12-13T16:02:44.159814Z","shell.execute_reply.started":"2023-12-13T16:02:28.061612Z"},"id":"5cf58cf0-6ed5-4a2e-9bec-c835c95a1acf","outputId":"10fcc41f-2546-481d-c467-3f6a4a94afae","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.880\n","Precision: 0.888\n","Recall: 0.871\n","F1-Score: 0.880\n"]}],"source":["# to split our dataset into training and test sets\n","from sklearn.model_selection import train_test_split\n","\n","#import the logistic regression from sklearn\n","from sklearn.linear_model import LogisticRegression\n","\n","# import evaluation metrics\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Train a logistic regression classifier\n","clf = LogisticRegression(max_iter=1000)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the classifier\n","print(\"Accuracy: %.3f\" %(accuracy_score(y_test, y_pred)))\n","print(\"Precision: %.3f\" %(precision_score(y_test, y_pred)))\n","print(\"Recall: %.3f\" %(recall_score(y_test, y_pred)))\n","print(\"F1-Score: %.3f\" %(f1_score(y_test, y_pred)))\n"]},{"cell_type":"markdown","metadata":{"id":"23675d74-8796-444b-94de-132d622f3464"},"source":["We observe that using BERT representations, our classifier can identify positive and sentiment reviews with an F1 score of 0.88!"]},{"cell_type":"markdown","metadata":{"id":"ba1b10bc-9ef4-45e1-a41e-01e2098d10a2"},"source":["#### 2.3. **Exercise:** Train a Logistic Regression classifier to perform the sentiment classification task but in this time, instead of using the mean of the word embeddings as the document embedding, use the Sentence tranformers DistilBERT to extract the features for each document."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T16:02:44.164027Z","iopub.status.busy":"2023-12-13T16:02:44.163500Z","iopub.status.idle":"2023-12-13T16:02:44.170349Z","shell.execute_reply":"2023-12-13T16:02:44.169092Z","shell.execute_reply.started":"2023-12-13T16:02:44.163978Z"},"id":"e28efae6-0648-4672-a47c-2b367e98eed8","trusted":true},"outputs":[],"source":["# Write your code here"]},{"cell_type":"markdown","metadata":{"id":"d327d806-d327-4fe3-b7f7-d482320d76a1"},"source":["### 3. Performing NLP Classification tasks by fine-tuning BERT models and training a classification head"]},{"cell_type":"markdown","metadata":{"id":"1fd3303c-4f07-446b-a63f-3d277ff93231"},"source":["What we have done thus far is to use the pre-trained BERT model to extract BERT representations, so we essentially did not update any weights on the model based on our annotated dataset of IMDB movie reviews."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T16:03:13.920744Z","iopub.status.busy":"2023-12-13T16:03:13.919407Z","iopub.status.idle":"2023-12-13T16:19:36.793749Z","shell.execute_reply":"2023-12-13T16:19:36.792672Z","shell.execute_reply.started":"2023-12-13T16:03:13.920693Z"},"id":"d316aa79-28f4-4890-b030-a9fb329cad1b","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51d48d33fada49c4be2f4f46d8631f39","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/32 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a766bf182a44b29a8d4a6b1e17b0484","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2000 09:17, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.223000</td>\n","      <td>0.220685</td>\n","      <td>0.916375</td>\n","      <td>0.916490</td>\n","      <td>0.905972</td>\n","      <td>0.927254</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 00:43]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.22068540751934052, 'eval_accuracy': 0.916375, 'eval_f1': 0.9164898264885782, 'eval_precision': 0.9059723593287266, 'eval_recall': 0.9272543571608992, 'eval_runtime': 43.4892, 'eval_samples_per_second': 183.954, 'eval_steps_per_second': 11.497, 'epoch': 1.0}\n"]}],"source":["# Necessary Imports\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_dataset\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = logits.argmax(axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n","    acc = accuracy_score(labels, predictions)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","# For GPU: Remvoe this line and to(device) in model definition. also change no_cuda to False in Training arguments\n","#device = torch.device('cpu')\n","\n","\n","# Split data into training and test set\n","train_df = df.sample(frac=0.8, random_state=42)  # 80% for training\n","test_df = df.drop(train_df.index)\n","\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","# Load the BERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Tokenize the dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n","\n","tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n","\n","# Load the BERT model for sequence classification\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device) # 2 labels: pos and neg\n","\n","#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device) # 2 labels: pos and neg\n","\n","# Define training arguments and set up Trainer\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=1,\n","    evaluation_strategy=\"epoch\",\n","    logging_dir=\"./logs\",\n","    logging_steps=500,\n","    do_train=True,\n","    do_eval=True,\n","    #no_cuda=True,\n","    load_best_model_at_end=True,\n","    save_strategy=\"epoch\",\n","    report_to=\"tensorboard\", # for logging to TensorBoard\n","    output_dir=\"./results\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"b3d52d1b-a875-4677-83b7-6fde8064f252"},"source":["We observe that by fine-tuning our BERT model and performing the classification task using the classification head on the BERT model that we improve the pefromacne substantially. The BERT-based classifier achieves an F1 score of 0.91! This is a substantially better classifier than the one we implemented above that was using Logistic Regression on the pre-trained BERT embeddings (F1 score of 0.88). Also, keep in mind that here we only fine-tuned for 1 epoch to speed-up the process. Usually we fine-tune for more epochs and pick the best-performing model after each epoch. So a way to improve performance is fine-tune the model for more epochs and pick the top performing model as the classifier."]},{"cell_type":"markdown","metadata":{"id":"29f27abd-9b24-434e-800b-91eda982107a"},"source":["## TI3160TU: Natural Language Processing - Transformers and BERT models Lab -- END"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4159842,"sourceId":7193424,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
